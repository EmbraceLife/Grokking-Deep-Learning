{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Anyone-Can-Learn-To-Code-an-LSTM-RNN-in-Python-(Part-1:-RNN)\" data-toc-modified-id=\"Anyone-Can-Learn-To-Code-an-LSTM-RNN-in-Python-(Part-1:-RNN)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)</a></div><div class=\"lev2 toc-item\"><a href=\"#Toy-code-File\" data-toc-modified-id=\"Toy-code-File-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Toy code File</a></div><div class=\"lev2 toc-item\"><a href=\"#How-RNN-memorize-in-graphs\" data-toc-modified-id=\"How-RNN-memorize-in-graphs-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>How RNN memorize in graphs</a></div><div class=\"lev3 toc-item\"><a href=\"#h_layer-vs-input_layer-recurrent\" data-toc-modified-id=\"h_layer-vs-input_layer-recurrent-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>h_layer vs input_layer recurrent</a></div><div class=\"lev3 toc-item\"><a href=\"#h_layer-recurrent-structure\" data-toc-modified-id=\"h_layer-recurrent-structure-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>h_layer recurrent structure</a></div><div class=\"lev3 toc-item\"><a href=\"#RNN-forward-and-backward-passes\" data-toc-modified-id=\"RNN-forward-and-backward-passes-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>RNN forward and backward passes</a></div><div class=\"lev2 toc-item\"><a href=\"#Diving-into-code\" data-toc-modified-id=\"Diving-into-code-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Diving into code</a></div><div class=\"lev3 toc-item\"><a href=\"#Import\" data-toc-modified-id=\"Import-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Import</a></div><div class=\"lev3 toc-item\"><a href=\"#Sigmoid-and-derivative\" data-toc-modified-id=\"Sigmoid-and-derivative-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Sigmoid and derivative</a></div><div class=\"lev3 toc-item\"><a href=\"#training-dataset-generation\" data-toc-modified-id=\"training-dataset-generation-133\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>training dataset generation</a></div><div class=\"lev3 toc-item\"><a href=\"#Model-parameters-and-initial-weights\" data-toc-modified-id=\"Model-parameters-and-initial-weights-134\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Model parameters and initial weights</a></div><div class=\"lev3 toc-item\"><a href=\"#Forward-pass\" data-toc-modified-id=\"Forward-pass-135\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Forward pass</a></div><div class=\"lev3 toc-item\"><a href=\"#Backward-pass\" data-toc-modified-id=\"Backward-pass-136\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Backward pass</a></div><div class=\"lev3 toc-item\"><a href=\"#Update-weights-and-print-progress\" data-toc-modified-id=\"Update-weights-and-print-progress-137\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Update weights and print progress</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)\n",
    "Baby steps to your neural network's first memories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- Trask's LSTM post [link](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)     \n",
    "- How binary addition work [video](https://www.youtube.com/watch?v=ypqYoFbPfTk)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Toy code File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:16:45.328649Z",
     "start_time": "2017-03-10T23:16:45.318719+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting anyone_can_code_lstm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile anyone_can_code_lstm.py\n",
    "\n",
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\",  str(overallError))\n",
    "        print(\"Pred:\" , str(d))\n",
    "        print(\"True:\" , str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int), \" + \" , str(b_int) , \" = \" , str(out))\n",
    "        print(\"------------\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:16:52.289061Z",
     "start_time": "2017-03-10T23:16:45.943252+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [ 3.45638663]\n",
      "Pred: [0 0 0 0 0 0 0 1]\n",
      "True: [0 1 0 0 0 1 0 1]\n",
      "9  +  60  =  1\n",
      "------------\n",
      "Error: [ 3.63389116]\n",
      "Pred: [1 1 1 1 1 1 1 1]\n",
      "True: [0 0 1 1 1 1 1 1]\n",
      "28  +  35  =  255\n",
      "------------\n",
      "Error: [ 3.91366595]\n",
      "Pred: [0 1 0 0 1 0 0 0]\n",
      "True: [1 0 1 0 0 0 0 0]\n",
      "116  +  44  =  72\n",
      "------------\n",
      "Error: [ 3.72191702]\n",
      "Pred: [1 1 0 1 1 1 1 1]\n",
      "True: [0 1 0 0 1 1 0 1]\n",
      "4  +  73  =  223\n",
      "------------\n",
      "Error: [ 3.5852713]\n",
      "Pred: [0 0 0 0 1 0 0 0]\n",
      "True: [0 1 0 1 0 0 1 0]\n",
      "71  +  11  =  8\n",
      "------------\n",
      "Error: [ 2.53352328]\n",
      "Pred: [1 0 1 0 0 0 1 0]\n",
      "True: [1 1 0 0 0 0 1 0]\n",
      "81  +  113  =  162\n",
      "------------\n",
      "Error: [ 0.57691441]\n",
      "Pred: [0 1 0 1 0 0 0 1]\n",
      "True: [0 1 0 1 0 0 0 1]\n",
      "81  +  0  =  81\n",
      "------------\n",
      "Error: [ 1.42589952]\n",
      "Pred: [1 0 0 0 0 0 0 1]\n",
      "True: [1 0 0 0 0 0 0 1]\n",
      "4  +  125  =  129\n",
      "------------\n",
      "Error: [ 0.47477457]\n",
      "Pred: [0 0 1 1 1 0 0 0]\n",
      "True: [0 0 1 1 1 0 0 0]\n",
      "39  +  17  =  56\n",
      "------------\n",
      "Error: [ 0.21595037]\n",
      "Pred: [0 0 0 0 1 1 1 0]\n",
      "True: [0 0 0 0 1 1 1 0]\n",
      "11  +  3  =  14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "!python anyone_can_code_lstm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## How RNN memorize in graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### h_layer vs input_layer recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:11:55.400349Z",
     "start_time": "2017-03-10T23:11:55.394485+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint. [go to post](https://hyp.is/gS607gYEEee3C58d8pc57g/iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "\n",
    "> The network REALLY needs to know more about what part of the song its in. However, the \"hidden layer recurrence\" doesn't break down in this way. It subtely remembers everything it saw (with memories becoming more subtle as it they fade into the past) [go to post](https://hyp.is/OmTjJgYEEeeIQ_MJ-s_YGQ/iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:11:55.416144Z",
     "start_time": "2017-03-10T23:11:55.403461+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/7WNccZ2lMT0YfSkPi1F4dzreHCJ8D7mbKnB6K95hR16Gko7yhbUvp8hGbDW7-N5oVfoi98T0eUYbvdcDEbPsYydMC6B33DFV0b9cubajxHhsyG8AcmpgOx2jMOuiOQ0jXfFSLOUgdaBpu2z9ZRPBFQVnzKcBB80ZmLFjD8IMIpGsOvkT551b4IDKhrM10AyIoPPkoq550_yVZIPPIp6cy_GsI3pOlaxFJrtZTHZeJM8GW05DsL4VpDX1zeMCThdLgPhiSfgCKS6we_WMQqhI_KDvqiR-znFt4rpluIGCoWHeX-W1XLMlWNZVgMFVSOgrSburKNdFaIHKwDTZjZ819_3_gvEMNkCPhHJpBchxgG2-13PZ-YAhWxqPlV_5mMknk79RYt-bfcverJrPnK2O0N2qRbLRXT9Ni4tlRU9eP9DZwnEBeyHkke_BMU8TqslhUt3mehZHW4TkzcaH09kj2phHFbdI38zwPdR0wtrGDSUOUL-cHLmu1tlP6N5ulgxIfVkKVyAnoUkU0RDx1dDhM99JMn2GHxT2q4aYHWgIn3ED4q83QoVLhdlFdteXedaPOEDrpoNyiTQKAPc5PIDC7PBwSlqKTGkRTaeS8DSMIV0c0oD2Tcqa=w1546-h884-no\" width=\"800\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width = 800, height= 500, url='https://lh3.googleusercontent.com/7WNccZ2lMT0YfSkPi1F4dzreHCJ8D7mbKnB6K95hR16Gko7yhbUvp8hGbDW7-N5oVfoi98T0eUYbvdcDEbPsYydMC6B33DFV0b9cubajxHhsyG8AcmpgOx2jMOuiOQ0jXfFSLOUgdaBpu2z9ZRPBFQVnzKcBB80ZmLFjD8IMIpGsOvkT551b4IDKhrM10AyIoPPkoq550_yVZIPPIp6cy_GsI3pOlaxFJrtZTHZeJM8GW05DsL4VpDX1zeMCThdLgPhiSfgCKS6we_WMQqhI_KDvqiR-znFt4rpluIGCoWHeX-W1XLMlWNZVgMFVSOgrSburKNdFaIHKwDTZjZ819_3_gvEMNkCPhHJpBchxgG2-13PZ-YAhWxqPlV_5mMknk79RYt-bfcverJrPnK2O0N2qRbLRXT9Ni4tlRU9eP9DZwnEBeyHkke_BMU8TqslhUt3mehZHW4TkzcaH09kj2phHFbdI38zwPdR0wtrGDSUOUL-cHLmu1tlP6N5ulgxIfVkKVyAnoUkU0RDx1dDhM99JMn2GHxT2q4aYHWgIn3ED4q83QoVLhdlFdteXedaPOEDrpoNyiTQKAPc5PIDC7PBwSlqKTGkRTaeS8DSMIV0c0oD2Tcqa=w1546-h884-no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### h_layer recurrent structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- how to calc hidden layer using input and previous hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:11:55.428338Z",
     "start_time": "2017-03-10T23:11:55.418867+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://iamtrask.github.io/img/basic_recurrence_singleton.png\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width = 500, height=300, url='https://iamtrask.github.io/img/basic_recurrence_singleton.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### RNN forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:11:55.441068Z",
     "start_time": "2017-03-10T23:11:55.431894+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://iamtrask.github.io/img/recurrence_gif.gif\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width = 500, height = 300, url='https://iamtrask.github.io/img/recurrence_gif.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T15:11:55.453933Z",
     "start_time": "2017-03-10T23:11:55.444728+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://iamtrask.github.io/img/backprop_through_time.gif\" width=\"500\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width = 500, height = 300, url='https://iamtrask.github.io/img/backprop_through_time.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Diving into code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:08:52.286426Z",
     "start_time": "2017-03-11T08:08:51.909420+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import copy, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T12:17:23.023200Z",
     "start_time": "2017-03-10T20:17:23.018370+08:00"
    },
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Sigmoid and derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:08:56.695041Z",
     "start_time": "2017-03-11T08:08:56.686430+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### training dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:08:57.759543Z",
     "start_time": "2017-03-11T08:08:57.753777+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)     # 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:08:59.178628Z",
     "start_time": "2017-03-11T08:08:59.171412+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# see transformation below\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T12:13:19.791450Z",
     "start_time": "2017-03-10T20:13:19.777084+08:00"
    },
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "variables": {
     "np.array([range(largest_number)],dtype=np.uint8)": "<p><strong>NameError</strong>: name &#39;np&#39; is not defined</p>\n",
     "np.array([range(largest_number)],dtype=np.uint8).T": "<p><strong>NameError</strong>: name &#39;np&#39; is not defined</p>\n",
     "np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T, axis=1)": "<p><strong>NameError</strong>: name &#39;np&#39; is not defined</p>\n"
    }
   },
   "source": [
    "binary_1 == {{np.array([range(largest_number)],dtype=np.uint8)}}     \n",
    "\n",
    "binary_2 == {{np.array([range(largest_number)],dtype=np.uint8).T}}       \n",
    "\n",
    "binary == {{np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T, axis=1)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:09:00.930776Z",
     "start_time": "2017-03-11T08:09:00.925296+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Model parameters and initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:09:10.646979Z",
     "start_time": "2017-03-11T08:09:10.642019+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# input variables\n",
    "alpha = 0.1\n",
    "                                                        # input layer offer 2 values\n",
    "input_dim = 2\n",
    "                                                        # hidden layer output 16 values\n",
    "hidden_dim = 16\n",
    "                                                        # output just a single value\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T00:09:12.040653Z",
     "start_time": "2017-03-11T08:09:12.031349+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "# update to 0s\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-10T12:29:44.511460Z",
     "start_time": "2017-03-10T20:29:44.497239+08:00"
    },
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "variables": {
     "synapse_0": "<p><strong>NameError</strong>: name &#39;synapse_0&#39; is not defined</p>\n",
     "synapse_0_update": "<p><strong>NameError</strong>: name &#39;synapse_0_update&#39; is not defined</p>\n"
    }
   },
   "source": [
    "synapse_0 == {{synapse_0}}\n",
    "\n",
    "synapse_0_update == {{synapse_0_update}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T01:20:35.747855Z",
     "start_time": "2017-03-11T09:20:35.600010+08:00"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [0 1 1 1 0 0 0 1]\n",
      "b: [0 0 0 0 0 0 0 0]\n",
      "c: [0 1 1 1 0 0 0 1]\n",
      "d: [0 0 0 0 0 0 0 0]\n",
      "-----------------------------------\n",
      "position: 0\n",
      "--------------\n",
      "X: [[1 0]]\n",
      "y: [[1]]\n",
      "layer_1 [[ 0.52438739  0.6059641   0.55120158  0.52242653  0.46190139  0.57243386\n",
      "   0.46883406  0.68644385  0.71653238  0.44198326  0.64186088  0.51444344\n",
      "   0.53396987  0.7008174   0.29777245  0.30454628]]\n",
      "layer_2 [[ 0.52385887]]\n",
      "layer_2_deltas: [array([[ 0.11876424]])]\n",
      "overallError: [ 0.47614113]\n",
      "d: [0 0 0 0 0 0 0 1]\n",
      "layer_1_values: [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.]), array([[ 0.52438739,  0.6059641 ,  0.55120158,  0.52242653,  0.46190139,\n",
      "         0.57243386,  0.46883406,  0.68644385,  0.71653238,  0.44198326,\n",
      "         0.64186088,  0.51444344,  0.53396987,  0.7008174 ,  0.29777245,\n",
      "         0.30454628]])]\n",
      "position: 1\n",
      "--------------\n",
      "X: [[0 0]]\n",
      "y: [[0]]\n",
      "layer_1 [[ 0.4607088   0.70122245  0.91939146  0.10792181  0.90421724  0.19833045\n",
      "   0.15515846  0.49971006  0.64251161  0.68349939  0.51897404  0.49447328\n",
      "   0.2172712   0.30676424  0.55157391  0.17998088]]\n",
      "layer_2 [[ 0.26743782]]\n",
      "layer_2_deltas: [array([[ 0.11876424]]), array([[-0.05239503]])]\n",
      "overallError: [ 0.74357895]\n",
      "d: [0 0 0 0 0 0 0 1]\n",
      "layer_1_values: [array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.]), array([[ 0.52438739,  0.6059641 ,  0.55120158,  0.52242653,  0.46190139,\n",
      "         0.57243386,  0.46883406,  0.68644385,  0.71653238,  0.44198326,\n",
      "         0.64186088,  0.51444344,  0.53396987,  0.7008174 ,  0.29777245,\n",
      "         0.30454628]]), array([[ 0.4607088 ,  0.70122245,  0.91939146,  0.10792181,  0.90421724,\n",
      "         0.19833045,  0.15515846,  0.49971006,  0.64251161,  0.68349939,\n",
      "         0.51897404,  0.49447328,  0.2172712 ,  0.30676424,  0.55157391,\n",
      "         0.17998088]])]\n"
     ]
    }
   ],
   "source": [
    "# training logic\n",
    "                                                                    # num of epochs\n",
    "for j in range(1):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    \n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "    print(\"a:\", a)\n",
    "    \n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "    print(\"b:\", b)\n",
    "    \n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    print(\"c:\", c)\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "                                                                    # store predictions\n",
    "    d = np.zeros_like(c)\n",
    "    print(\"d:\", d)\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    \n",
    "                                                                    # initialize: an element of array_zeros (16,) as first element\n",
    "                                                                    # hidden_dim = 16\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "                                                                    # full dataset is 8 samples\n",
    "                                                                    # binary_dim = 8\n",
    "                                                                    # for 8 digit, one by one moving from right to left\n",
    "    count=0\n",
    "    for position in range(binary_dim):      \n",
    "        \n",
    "\n",
    "#         print('position:', position)\n",
    "#         print('--------------')\n",
    "        \n",
    "        # generate input and output\n",
    "                                                                    # X.shape dim(1, 2)\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "#         print('X:', X)\n",
    "#         print('y:', y)\n",
    "        \n",
    "\n",
    "                                                                    # layer_1_values[-1] always refer to previous layer_1_values    \n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "                             # input_X, W              previous_layer_1, W_h          : to keep memory\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "                                                                    # layer_1 return 16 values or 16 neurons\n",
    "#         print(\"layer_1\", layer_1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # output layer (new binary representation)\n",
    "                                                                    # layer_2 always refer a single value\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "#         print(\"layer_2\", layer_2)\n",
    "        \n",
    "        \n",
    "                                                                    # not sure what layer_2 is exactly doing\n",
    "                                                                    # we want network to do binary addtion\n",
    "                                                                    # backward pass will help us correct network to do addition\n",
    "                                                                    # iteration by iteration\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "                                                                    # store layer_2_delta: gradient?\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "#         print('layer_2_deltas:', layer_2_deltas)\n",
    "        \n",
    "                                                                    # add up error of a digit out of 8 into overallError\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "#         print(\"overallError:\", overallError)\n",
    "        \n",
    "        \n",
    "        # decode estimate so we can print it out\n",
    "                                                                    # store prediciton of each digit into d, from right to left\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "#         print('d:', d)\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "                                                                    # store hidden layer or layer_1, so that layer_1_values[-1]\n",
    "                                                                    # can always refer to previous hidden layer's value\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "#         print(\"layer_1_values:\", layer_1_values)\n",
    "  \n",
    "        if count < 2: \n",
    "            print('position:', position)\n",
    "            print('--------------')\n",
    "            print('X:', X)\n",
    "            print('y:', y)\n",
    "            print(\"layer_1\", layer_1)\n",
    "            print(\"layer_2\", layer_2)\n",
    "            print('layer_2_deltas:', layer_2_deltas)\n",
    "            print(\"overallError:\", overallError)\n",
    "            print('d:', d)\n",
    "            print(\"layer_1_values:\", layer_1_values)\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "                                                      # set layer_1_delta as 0s of 16\n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "\n",
    "    for position in range(binary_dim):\n",
    "                                                      # get a and b with the most left digit\n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        \n",
    "                                                      # get the previous layer_1 value\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "                                                      # get the previous previous layer_1 value  \n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "                                                      # get previous layer_2_delta\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        \n",
    "        \n",
    "        # error at hidden layer\n",
    "        # layer_1_delta has two parts: \n",
    "        #                        from future_layer_1_delta           from layer_2_delta,         \n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        \n",
    "        \n",
    "        # let's update all our weights \n",
    "                                                        # for each of 8 digits in loop\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        \n",
    "                                                        # current layer_1_delta becomes future layer_1_delta to next  \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Update weights and print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-11T02:20:32.682578Z",
     "start_time": "2017-03-11T10:20:32.671308+08:00"
    },
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "                                                        # update all weights at end of each epoch using updated part from \n",
    "                                                        # the forward and backward above\n",
    "    \n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "                                                        # get this part of weights back to 0s for next epoch\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    \n",
    "                                                        # print out progress or how well this model learned binary addition\n",
    "                                                        # at end of each 1000 epochs\n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        \n",
    "                                                        # reversed(d): make sure to loop digit from right to left\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "251px",
    "left": "971px",
    "right": "20px",
    "top": "77px",
    "width": "259px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
